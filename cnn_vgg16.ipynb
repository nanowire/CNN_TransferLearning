{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring VGG16 Convolutional Neural Network <a id=\"1\"></a> \n",
    "\n",
    "[VGG16](https://neurohive.io/en/popular-networks/vgg16/) is a well-established convolutional neural network (CNN) trained on millions of images for classification in 1000 object categories. VGG16 is publically available through keras, an API for tensorflow. We can readily import it, inspect its architecture, and ran some images into it and see what kind of object the network thinks it is. The following code blocks explore the functionality of VGG16. Note: in order to run the code in this notebook, you must have tensorflow, keras, and other supporting modules installed in your virtual environment. See README for instruction on how to install these dependencies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n03776460', 'mobile_home', 0.59778005)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "# Try using VGG16 directly on an image\n",
    "# The image is a single family home, and the network thinks that it is a mobile home, which is not too bad\n",
    "\n",
    "image = image.load_img('House3.jpg', target_size=(224, 224))\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = preprocess_input(image)\n",
    "y_hat = model.predict(image)\n",
    "label = decode_predictions(y_hat)[0][0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manipulating VGG16 Network Layers <a id=\"2\"></a>\n",
    "\n",
    "The next two blocks contain codes for eliminating the fully-connected layers and softmax layer from the VGG16 network, and inspect the resulting 7x7x512 tensor after we input a 224x224 (required input dimension for VGG16) color image to the network. \n",
    "\n",
    "Although not required for this project, we also played around with inserting layers into VGG16. The code is in [Section 8](#8) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use 'include_top=False' argument to not include the output layer\n",
    "\n",
    "vgg_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 512)\n",
      "[[[[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.         29.363773    0.         ...  0.         29.04313\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.06901628 ...  0.          7.9472775\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          2.3990252\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "# Try using VGG16 directly on an image without prediction layer\n",
    "\n",
    "vgg_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "image = image.load_img('House3.jpg', target_size=(224, 224))\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = preprocess_input(image)\n",
    "y_hat = vgg_model.predict(image)\n",
    "\n",
    "print(y_hat.shape)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN Image Batch Processing\n",
    "\n",
    "The next several blocks of code process all property images. There are ~ 700,000 images in total for interior and exterior from ~ 62,000 properties around the Boston area. For each image, the code extracts all the non-zero entries from an output tensor, record their indices, and write them into a dictionary along with the property's MLSNUM and IMGNUM.\n",
    "\n",
    "The dictionaries from all images are stored in a list, which is in turn written into a csv file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to only record the indices where the tensor has non-zero values\n",
    "# Doing this will save a lot of space when writing the feature vectors into a database\n",
    "\n",
    "def nonzeros(arr, filtered):\n",
    "    for index, item in enumerate(arr):\n",
    "        if type(item) is np.ndarray:\n",
    "            filtered[index] = nonzeros(item, dict())\n",
    "        else:\n",
    "            if item != 0:\n",
    "                filtered[index] = item\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  0  images that failed to load.\n",
      "(14, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURES</th>\n",
       "      <th>IMGNUM</th>\n",
       "      <th>MLSNUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{0: {0: {0: {4: 2.9935026, 28: 51.557457, 82: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{0: {0: {0: {17: 0.6998304, 25: 24.099962, 55:...</td>\n",
       "      <td>1</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{0: {0: {0: {6: 9.023701, 12: 29.83715, 20: 1....</td>\n",
       "      <td>10</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{0: {0: {0: {12: 4.6816025, 25: 17.246347, 26:...</td>\n",
       "      <td>11</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{0: {0: {0: {5: 5.2803173, 30: 54.870575, 58: ...</td>\n",
       "      <td>12</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{0: {0: {0: {4: 18.096224, 20: 10.33395, 25: 1...</td>\n",
       "      <td>13</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{0: {0: {0: {2: 8.998453, 4: 22.723763, 46: 15...</td>\n",
       "      <td>2</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{0: {0: {0: {40: 26.316399, 55: 1.3048272, 63:...</td>\n",
       "      <td>3</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{0: {0: {0: {0: 2.428958, 2: 1.8430979, 5: 4.1...</td>\n",
       "      <td>4</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{0: {0: {0: {1: 48.76469, 28: 30.675917, 37: 2...</td>\n",
       "      <td>5</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{0: {0: {0: {12: 23.78297, 15: 23.541634, 30: ...</td>\n",
       "      <td>6</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{0: {0: {0: {0: 4.593894, 5: 16.247684, 9: 21....</td>\n",
       "      <td>7</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{0: {0: {0: {6: 3.5438418, 12: 18.38065, 22: 8...</td>\n",
       "      <td>8</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{0: {0: {0: {12: 14.776984, 21: 3.5375664, 30:...</td>\n",
       "      <td>9</td>\n",
       "      <td>71524024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             FEATURES IMGNUM    MLSNUM\n",
       "0   {0: {0: {0: {4: 2.9935026, 28: 51.557457, 82: ...      0  71524024\n",
       "1   {0: {0: {0: {17: 0.6998304, 25: 24.099962, 55:...      1  71524024\n",
       "2   {0: {0: {0: {6: 9.023701, 12: 29.83715, 20: 1....     10  71524024\n",
       "3   {0: {0: {0: {12: 4.6816025, 25: 17.246347, 26:...     11  71524024\n",
       "4   {0: {0: {0: {5: 5.2803173, 30: 54.870575, 58: ...     12  71524024\n",
       "5   {0: {0: {0: {4: 18.096224, 20: 10.33395, 25: 1...     13  71524024\n",
       "6   {0: {0: {0: {2: 8.998453, 4: 22.723763, 46: 15...      2  71524024\n",
       "7   {0: {0: {0: {40: 26.316399, 55: 1.3048272, 63:...      3  71524024\n",
       "8   {0: {0: {0: {0: 2.428958, 2: 1.8430979, 5: 4.1...      4  71524024\n",
       "9   {0: {0: {0: {1: 48.76469, 28: 30.675917, 37: 2...      5  71524024\n",
       "10  {0: {0: {0: {12: 23.78297, 15: 23.541634, 30: ...      6  71524024\n",
       "11  {0: {0: {0: {0: 4.593894, 5: 16.247684, 9: 21....      7  71524024\n",
       "12  {0: {0: {0: {6: 3.5438418, 12: 18.38065, 22: 8...      8  71524024\n",
       "13  {0: {0: {0: {12: 14.776984, 21: 3.5375664, 30:...      9  71524024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This block of code processes a folder containing images to be analyzed, and store each image's \n",
    "# feature vector (the non-zero entries) into a dictionary  \n",
    "# Process a folder called 'temp' that contains only 14 images to validate the code \n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "import csv\n",
    "\n",
    "# Create regular expression pattern\n",
    "p = re.compile(\"(\\d{8})_img_(\\d+).jpg\")\n",
    "\n",
    "# Create an array to store image features\n",
    "data_arr = []\n",
    "failcount = 0\n",
    "\n",
    "for root, dirs, files in os.walk('C:\\\\temp'):\n",
    "    for name in files:\n",
    "        match = p.match(name)\n",
    "        if match:\n",
    "            try:\n",
    "                img_dict = {\n",
    "                    'MLSNUM': match.group(1),\n",
    "                    'IMGNUM': match.group(2)\n",
    "                }\n",
    "                path = os.path.join(root, name)\n",
    "                img = image.load_img(path, target_size=(224, 224))\n",
    "                img_data = img_to_array(img)\n",
    "                img_data = np.expand_dims(img_data, axis=0)\n",
    "                img_data = preprocess_input(img_data)\n",
    "                \n",
    "                features = vgg_model.predict(img_data)              # features is a numpy ndarray\n",
    "                img_dict['FEATURES'] = nonzeros(features, dict())   # store the non-zero entries into a nested dictionary\n",
    "                \n",
    "                data_arr.append(img_dict)                   # store {MLSNUM: IMGNUM: FEATURES:} as an element in a list\n",
    "            except OSError:\n",
    "                failcount += 1\n",
    "\n",
    "print('There were ', failcount, ' images that failed to load.')\n",
    "\n",
    "# make the data array into a data frame so we can look at it\n",
    "data_df = pd.DataFrame(data_arr)\n",
    "\n",
    "print(data_df.shape)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data_arr into a csv file \n",
    "\n",
    "import csv\n",
    "\n",
    "with open('temp.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "    fc = csv.DictWriter(output_file, \n",
    "                        fieldnames=data_arr[0].keys(),\n",
    "                       )\n",
    "    fc.writeheader()\n",
    "    fc.writerows(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data into a database\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "conn = psycopg2.connect(\"host=cnn-project3.cetsu4jwuaoc.us-east-1.rds.amazonaws.com dbname=fulldata port=5432 username=postgres password=JhuSli0d38oNNfAauam9\")\n",
    "current = conn.cursor()\n",
    "\n",
    "df_condos = pd.read_csv(\"temp.csv\", sep='|', index_col=False)\n",
    "\n",
    "for index, condo in data_df.iterrows():\n",
    "    current.execute(\n",
    "    \"'INSERT INTO photos (mlsnum, imgnum, features) VALUES (%s%s%s)'\",\n",
    "    (condo.MLSNUM, condo.IMGNUM, condo.FEATURES)\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "current.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing \"Similarity\" Between Two Images\n",
    "\n",
    "The next several blocks of code compute the \"similarity\" or \"distance\" between two image tensors obtained from VGG16 feature extraction. \n",
    "\n",
    "Algorithmatically, the following steps are used to compute similarity:\n",
    "1. An user's input image goes through VGG16 convoluitonal layers and the CNN outputs a 7x7x512 output tensor (a numpy array). \n",
    "2. The feature tensor from one image stored in our database is retrieved as a string, and the string is converted back to dictionary, and the dictionary is converted back to a numpy array\n",
    "3. The numpy array from 1. is subtracted from the numpy array from 2.\n",
    "4. The numpy norm() function is applied to the resulting numpy array from 3. to obtain the distance\n",
    "5. Repeat steps 1. to 4. for all images stored in the database\n",
    "6. Select the top 5 properties to display to user (5 smallest distance score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a dictionary back to its corresponding numpy array (recover all omitted zeroes) \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def backToArr(dict):\n",
    "    arr = np.zeros((1, 7, 7, 512))\n",
    "\n",
    "    for row in dict[0]:\n",
    "        for col in dict[0][row]: \n",
    "            for elem in dict[0][row][col]:\n",
    "                arr[0][row][col][elem] = dict[0][row][col][elem]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute norm(diff) from dictionary directly \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from numpy.linalg import norm\n",
    "\n",
    "user_input = '71524024_img_0.jpg'\n",
    "img = image.load_img(user_input, target_size=(224, 224))\n",
    "img_data = img_to_array(img)\n",
    "img_data = np.expand_dims(img_data, axis=0)\n",
    "img_data = preprocess_input(img_data)\n",
    "                \n",
    "features = vgg_model.predict(img_data)\n",
    "\n",
    "arr = backToArr(data_df['FEATURES'].iloc[0])\n",
    "\n",
    "norm(features - arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Issue in Precision Loss During Data Conversion and Transmission<a id=\"5\"></a>\n",
    "\n",
    "The previous code block demonstrates that the norm() function is working properly even if the input numpy array is multi-dimensional. For two identical images norm() outputs a distance of 0, as it should be. \n",
    "\n",
    "However, when our algorithm is run formally, minor precision loss is noticed after data conversion and transmission, such that the distance between two identical images are not exactly 0, but some small number. In the following paragraph, a double arrow, \"&rarr;&rarr;\", represents a VGG16 feature extraction step; a single arrow, \"&rarr;\", represents a single data conversion step. \n",
    "\n",
    "An input image from user outputs a numpy array: \n",
    "\n",
    "<font size=3>user input:  image &rarr;&rarr; numpy array</font>\n",
    "\n",
    "<u>But how a database feature tensor is handled affects precision!</u> We tested the following 3 scenarios:\n",
    "\n",
    "\n",
    "| <font size=2.5>Scenario</font> | <font size=2.5>Conversion Process</font> | <font size=2.5>Distance Computed by norm()</font> |\n",
    "| ------------------------------ | ----------------------------------------------- | --------------------------------------: |\n",
    "| <font size=2.5>Retrieve feature tensor directly without converting to string <br/> and without importing to database</font> | <font size=3>image &rarr;&rarr; numpy array &rarr; dict &rarr; numpy array</font> | <font size=3>$$0.0$$</font> |\n",
    "<h>\n",
    "| <font size=2.5>Feature tensor stored as dict is converted to string, then converted back to dict, <br/>then converted to numpy array</font> | <font size=3>image &rarr;&rarr; numpy array &rarr; dict <span style=\"color:red\">&rarr; string &rarr; dict</span> &rarr; numpy array</font> | <font size=3>$$2.28e^{-5}$$</font> |\n",
    "<h>\n",
    "| <font size=2.5>Feature tensor stored as dict is converted to string, then converted back to dict, then converted to numpy array, then converted to python list using tolist();<br/>(requires user input to also be converted to list)</font> | <font size=3>image &rarr;&rarr; numpy array &rarr; dict <span style=\"color:red\">&rarr; string &rarr; dict</span> &rarr; numpy array <span style=\"color:blue\">&rarr; list</span></font> | <font size=3>$$5.29e^{-11}$$</font> |\n",
    "\n",
    "<br/>\n",
    "    \n",
    "The above table indicates that conversion of the output numpy array to string causes a small loss of precision such that the distance computed is not exactly 0 anymore. However, using tolist() to convert the numpy array to list recovers some of the precision. When the data was imported to the database and retrieved, a relatively large precision loss is observed: \n",
    "\n",
    "| <font size=2.5>Scenario</font> | <font size=2.5>Conversion Process</font> | <font size=2.5>Distance Computed by norm()</font> |\n",
    "| ------------------------------ | ----------------------------------------------- | --------------------------------------: |\n",
    "| <font size=2.5>Retrieve feature tensor from the database (stored as string), converted back to dict, then converted to numpy array</font> | <font size=3>image &rarr;&rarr; numpy array &rarr; dict <span style=\"color:red\">&rarr; string &rarr; database &rarr; string &rarr; dict </span>&rarr; numpy array</font> | <font size=3>$$12 - 20$$ (depending on image)</font> |\n",
    "\n",
    "<br/>\n",
    "\n",
    "In essence, a trip to a database caused a significant loss in precision. We could implement tolist() to the final numpy array after retrieving the feature tensor from the database, and see if this will improve precision significantly like scenario 3 above. \n",
    "\n",
    "<font size=2.5>**However, as far as current application is concerned, loss of precision is does NOT alter the output result, that is, the most similar image will still have the lowest distance score among all images, since precision loss acts universally and consistently on all images.**</font> \n",
    "\n",
    "For example, an input image will give a distance score of ~ 15 with respect to an identical image stored in the database, whereas all other dissimilar images' distance scores are in the range of thousands.\n",
    "\n",
    "The following code blocks explore our observation of precision loss, and explains a strategy by which we could keep this phenomenon to a minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2809222259403666e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate the real process when retrieving feature tensor as a string from database\n",
    "# Compute norm(diff) by first converting features to string then converting back to dictionary \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from numpy.linalg import norm\n",
    "from ast import literal_eval\n",
    "\n",
    "a = str(data_df['FEATURES'].iloc[0])\n",
    "b = literal_eval(a)\n",
    "arr = backToArr(b)\n",
    "\n",
    "img = image.load_img('71524024_img_0.jpg', target_size=(224, 224))\n",
    "img_data = img_to_array(img)\n",
    "img_data = np.expand_dims(img_data, axis=0)\n",
    "img_data = preprocess_input(img_data)\n",
    "                \n",
    "features = vgg_model.predict(img_data)\n",
    "\n",
    "# We lost some precision during conversions\n",
    "norm(features - arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25088\n",
      "25088\n",
      "25088\n",
      "5.20260620078875e-10\n",
      "5.2910455076166615e-11\n"
     ]
    }
   ],
   "source": [
    "# What if we convert the output numpy array into a python list?\n",
    "\n",
    "#c = backToArr(data_arr[0]['FEATURES']).tolist()[0]\n",
    "c = arr.tolist()[0]\n",
    "d = []\n",
    "\n",
    "for i in range(len(c)):\n",
    "    for j in range(len(c[i])):\n",
    "        for k in range(len(c[i][j])):\n",
    "            d.append(c[i][j][k])\n",
    "\n",
    "e = features.tolist()[0]\n",
    "f = []\n",
    "\n",
    "for i in range(len(e)):\n",
    "    for j in range(len(e[i])):\n",
    "        for k in range(len(e[i][j])):\n",
    "            f.append(e[i][j][k])\n",
    "\n",
    "print(len(d))\n",
    "print(len(f))\n",
    "\n",
    "g = []\n",
    "for i in range(len(d)):\n",
    "    g.append((d[i] - f[i])*(d[i] - f[i]))\n",
    "\n",
    "print(len(g))\n",
    "\n",
    "# significant improvement in precision!\n",
    "print(sum(g))\n",
    "print(norm(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion<a id=\"6\"></a>\n",
    "\n",
    "* VGG16 can be used not only for image classification (supervised learning) but also for unsupervised learning such as image feature extraction as we successfully demonstrated in this project\n",
    "* Feature tensors extracted can be regarded as a high-dimensional vector (i.e., a point in a high-dimensional space), and a similar image is expected to have similar feature tensor element-wise\n",
    "* The numpy norm() function is a good function to use for computing the \"distance\" between two images, since norm() is able to accept multi-dimension tensor and computes the norm without having to reduce the tensor to an array\n",
    "* It turns out that serialization among different data types can cause a slight loss in precision: \n",
    "   * One remedy is to convert the numpy array to list, which recovers some of the precision as demonstrated in [Section 5](#5)\n",
    "   * The loss in precision does not affect the matching process becase the loss in precision acts universally and consistently on every image\n",
    "* Computation time is huge if an input image is to compare with every feature tensor stored in the database (~ 700,000 of them), far exceeding the within 2 seconds rule if we are to deploy it as a fully functional app, some suggestion for improving this:\n",
    "   * Use PCA to reduce the dimension of the feature tensor (at the moment every image has 7x7x512 = 25,088 features)\n",
    "   * Apply some type of clustering to the imges stored in our database so that not all feature tensors have to be retrieved during computaiton time\n",
    "   * Upgrade hardware infrastructure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Future Directions<a id=\"7\"></a>\n",
    "\n",
    "#### Reduction of Dimensionality\n",
    "Feature extraction from VGG16 network outputs a 7x7x512 = 25088 features for a given image. As we saw from [Section 2](#2) that the output tensor is sparse, i.e., most of the entries are 0. This implies that there are a lot of redundant features in the output. It would be computationally more efficient if we can transform the high-dimensional feature space (25088-dimensional) into a lower dimensional space. \n",
    "\n",
    "Principal component Analysis (PCA) is the most widely used unsupervised learning technique to reduce feature space. Both matplotlib.mlab and sklearn offer a PCA module. \n",
    "\n",
    "#### Further Investigate the Issue of Precision Loss\n",
    "Flattening the output numpy array using tolist() seems to help a lot with precision, so it is worth investigating the possibility of implementing approach in our image process pipeline without compromising runtime. \n",
    "\n",
    "#### Enhance Visual Appeal of the App\n",
    "Due to limited time and the time investment in investigating the issue of precision loss, current app does not have a good visual appeal. We will enhance the visual appeal of this app in the near future. \n",
    "\n",
    "#### Reduce Computational Tasks for an Input Image\n",
    "At its current implementation in full scale, our app computes the distance between the input image feature tensor with every feature tensor stored in our database, which is ~ 700,000. Computation time is fairly long to run this computation. If we can somehow segment out just the relevant images to process it can save a lot of time. For example, if the input image is a kitchen, we only computes distances of the input image to all kitchen images in our database. This require prior clustering of our database images. [Places 365](http://places2.csail.mit.edu/) from MIT specializes in classifying images pertaining to places, and can be applied for this purpose.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Miscellaneous Exercises<a id=\"8\"></a>\n",
    "\n",
    "The next several blocks of code were used during initial exploration or were written in anticipation of the next phase in the image process pipeline. However, at the end they were not used in the development of the application. Feel free to play around the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for downloading images by looping through url's stored in the last column of the master csv file \n",
    "\n",
    "import urllib\n",
    "\n",
    "with open(\"XX.csv\", 'r') as csvfile:\n",
    "    i = 0\n",
    "    for line in csvfile:\n",
    "        splitted_line = line.split(\",\")\n",
    "        if splitted_line[38] != '' and splitted_line[38] != \\n:\n",
    "            urllib.urlretrieve(splitted_line[38], \"img\" + str(splitted_line[0]) + \".jpg\")\n",
    "            i += 1\n",
    "        else:\n",
    "            print(\"No result for index \" + splitted_line[0] + \"with mls number \" + splitted_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VGG16 as feature extractor: remove the last softmax (prediction) layer\n",
    "\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "x = model2.output\n",
    "x = Flatten(name = 'flatten')(x)\n",
    "x = Dense(128, activation='relu', name='fc1')(x)\n",
    "out = Dense(128, activation='relu', name='fc2')(x)\n",
    "custom_vgg_model = Model(image_input, out)\n",
    "\n",
    "print(custom_vgg_model.summary())\n",
    "\n",
    "#prediction = Dense(num_classes, activation = 'softmax')(out)\n",
    "#print(custom_vgg_model.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 as feature extractor: include every VGG16 layers except the last softmax layer\n",
    "\n",
    "\n",
    "model3 = VGG16(input_tensor = image_input, weights = 'imagenet', include_top = True)\n",
    "\n",
    "out = model3.get_layer('fc2').output\n",
    "\n",
    "custom_vgg_model2 = Model(image_input, out)\n",
    "\n",
    "print(custom_vgg_model2.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
